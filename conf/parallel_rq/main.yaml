seed: 42
n_query: 4
code_num: 256
quantizer_type: "rqvae"
project_name: "Parallel_RQ_Transformer"

exp_name: ???  # To be set from shell script
output_dir: "./log/${project_name}/${dataset.name}/${quantizer_type}/${exp_name}"
enable_clearml: true

dataset: 
  data_path: "data/amazon18/"
  name: "Arts"
  index_file: ".index_lemb_${quantizer_type}_${code_num}.json"
  image_index_file: ".index_vitemb_${quantizer_type}_${code_num}.json"
  num_id_tokens: 1 # Number of identical id tokens to represent items in the ID injection task
  max_his_len: 20
  train_data_mode: 0 # 0 for all data, 1 for only responses
  his_sep: ", " # Separator for history
  train_prompt_sample_num: 1 # Number of sampling prompts for each task
  add_prompt: False # Add prompt to the input

train: 
  batch_size: 1024 # 2048 slightly too large for 2 V100 GPUs on the Arts dataset
  batch_size_eval: 2048
  learning_rate: 1e-2
  weight_decay: 0.01
  lr_scheduler: "cosine"
  warmup_steps: 1000
  early_stopping_patience: 10
  max_epochs: 100
  gradient_accumulation_steps: 2
  fp16: false
  bf16: false
  check_val_every_n_epoch: 1
  save_and_eval_steps: 200
  logging_step: 50
  limit_val_batches: 1.0 # full validation 
  tasks: "seqrec" # Downstream tasks, separate by comma
  valid_task: "seqrec"
  training_mode: "standard"

  # # optim: "adamw_torch"
  # model_max_length: 2048
  # resume_from_checkpoint: null  # either training checkpoint or final adapter
  # warmup_ratio: 0.0 #0.01
  # deepspeed: "./config/ds_z3_bf16.json"
  
  # # REMOVE THEM FOR NOW
  # normalization_method: "per_sample" # normalization method for the masked training ["per_sample", "per_batch", "unnormalized"]

dataloader:
  num_workers: 8

model:
  # Similar values to TIGER model
  dim: 128
  max_spatial_seq_len: ${dataset.max_his_len}
  spatial_layers: 4
  depth_layers: 4
  dim_head: 64
  heads: 6
  attn_dropout: 0.1
  ff_mult: 4
  ff_dropout: 0.1
  attention_type: "full"

infer: 
  metrics: "hit@1,hit@5,hit@10,ndcg@5,ndcg@10"  # test metrics, separate by comma
  generation_mode: "parallel_beam_search" # 'parallel_beam_search' or 'autoregressive_beam_search'
  batch_size: 2048
  num_beams: 20
  test_task: "seqrec"
  sample_num: -1  # test sample number, -1 represents using all test data
  filter_items: true
  use_constraints: true
  first_token_constraints_path: "${dataset.data_path}/${dataset.name}/fast_first_token_constraints_codenum${code_num}_${quantizer_type}_.pt"
  transition_constraints_t1_path: "${dataset.data_path}/${dataset.name}/fast_transition_constraints_t1_codenum${code_num}_${quantizer_type}_.pt"
  transition_constraints_t2_path: "${dataset.data_path}/${dataset.name}/fast_transition_constraints_t2_codenum${code_num}_${quantizer_type}_.pt"
  prefix_to_uidx_t3_path: "${dataset.data_path}/${dataset.name}/prefix_to_uidx_t3_codenum${code_num}_${quantizer_type}_.pt"
  uidx_to_next_tokens_t3_path: "${dataset.data_path}/${dataset.name}/uidx_to_next_tokens_t3_codenum${code_num}_${quantizer_type}_.pt"

#   results_file: "${infer.output_dir}/results_${infer.test_task}_${infer.num_beams}.json"
#   save_file: "${infer.output_dir}/save_${infer.test_task}_${infer.num_beams}.json"
#   debug: false
  



hydra:
  run:
    dir: "./outputs/${project_name}/${dataset.name}/${quantizer_type}/${exp_name}/run/${now:%Y-%m-%d_%H-%M-%S}"
  sweep:
    dir: "./outputs/${project_name}/${dataset.name}/${quantizer_type}/${exp_name}/multirun/${now:%Y-%m-%d_%H-%M-%S}"
    subdir: v${hydra.job.num}_${hydra.job.override_dirname}
  job_logging:
    version: 1
    root:
      handlers: [console, file]
      level: DEBUG
    handlers:
      console:
        class: logging.StreamHandler
        formatter: simple
      # file:
      #   class: logging.FileHandler
      #   formatter: simple
      #   filename: main_job.log
    formatters:
      simple:
        format: "[%(asctime)s] [%(name)s] [%(levelname)s] - %(message)s"