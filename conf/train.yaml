train: 
  experiment_name: "${exp_name}_train"
  optim: "adamw_torch"
  learning_rate: 1e-3
  batch_size: 1024
  micro_batch_size: 512
  logging_step: 50
  model_max_length: 2048
  weight_decay: 0.01
  resume_from_checkpoint: null  # either training checkpoint or final adapter
  warmup_ratio: 0.0 #0.01
  save_and_eval_strategy: "epoch"
  save_and_eval_steps: 200
  fp16: false
  bf16: false
  gradient_checkpointing: false
  deepspeed: "./config/ds_z3_bf16.json"
  patient: 10
  tasks: "seqrec" # Downstream tasks, separate by comma
  valid_task: "seqrec"

  training_mode: "standard"
  normalization_method: "per_sample" # normalization method for the masked training ["per_sample", "per_batch", "unnormalized"]

  num_epochs: 200
  max_steps: -1  # if > 0, overrides num_epochs
  cutoff_len: 4096
  val_set_size: 2000
  lr_scheduler: "cosine"
  warmup_steps: 100
  group_by_length: false

  enable_clearml: true
  is_pretrained_model: true
  log_codebook_losses: true

model:
  # Model architecture hyperparameters if training from scratch
  # The default values match Hugging Face's T5Config for the T5-small model.
  d_model: 512
  d_kv: 64
  d_ff: 2048
  num_layers: 6
  num_heads: 8
  relative_attention_num_buckets: 32
  relative_attention_max_distance: 128
  dropout_rate: 0.1
  layer_norm_epsilon: 1e-6

  tie_word_embeddings: false # argument to tie input (`shared`) and output (`lm_head`) projection layers, in the single head case
  has_relative_encoder_item_bias: true
  has_relative_encoder_codebook_bias: true
  has_relative_decoder_item_bias_sa: true
  has_relative_decoder_codebook_bias_sa: true
  has_relative_decoder_item_bias_ca: false
  has_relative_decoder_codebook_bias_ca: false
  encoder_aggregation: "sum" # "flatten" (encoder sees every item token), "sum", "mean", or "concat"
  use_multi_head: false # false for single head, true for multiple projection heads (one per token/query)


hydra:
  run:
    dir: ./outputs/${dataset.name}/${quantizer_type}/parallel_TIGER/${exp_name}/train/${now:%Y-%m-%d_%H-%M-%S}
  
  job_logging:
    version: 1
    root:
      handlers: [console, file]
      level: DEBUG
    handlers:
      console:
        class: logging.StreamHandler
        formatter: simple
      file:
        class: logging.FileHandler
        formatter: simple
        filename: ${hydra.run.dir}/train_job.log
    formatters:
      simple:
        format: "[%(asctime)s] [%(name)s] [%(levelname)s] - %(message)s"