from typing import Dict, List, Set, cast
import string
from transformers import PreTrainedTokenizerFast
from tokenizers import Tokenizer
from tokenizers.models import WordLevel
from tokenizers.pre_tokenizers import Whitespace


class CustomT5Tokenizer(PreTrainedTokenizerFast):
    """
    Custom T5 tokenizer class that extends PreTrainedTokenizerFast.
    
    This tokenizer is specifically designed for T5-based models with custom vocabulary
    containing special tokens for code representations.
    
    Attributes:
        model_input_names: List of input names required by the model.
    """
    model_input_names = ["input_ids", "attention_mask"]


def get_all_tokens(code_num: int = 256, num_prefixes: int = 4) -> List[str]:
    """
    Generate all custom tokens for the vocabulary.
    
    Creates tokens with a specified number of different prefixes using letters
    of the alphabet in order (a_, b_, c_, etc.) and numbers from 0 to code_num-1 
    for each prefix.
    
    Args:
        code_num: The number of codes to generate for each prefix. Defaults to 256.
        num_prefixes: The number of different prefixes to use. It corresponds to the Semantic ID number of token.
                     Defaults to 4. Must be between 1 and 26 (inclusive).

    Returns:
        A sorted list of all generated tokens.
        
    Raises:
        ValueError: If num_prefixes is not between 1 and 26 (inclusive).
        
    Example:
        >>> get_all_tokens(3, 2)
        ['<a_0>', '<a_1>', '<a_2>', '<b_0>', '<b_1>', '<b_2>']
    """
    if not 1 <= num_prefixes <= 26:
        raise ValueError(f"num_prefixes must be between 1 and 26 (inclusive), got {num_prefixes}")
    
    # Generate prefixes using lowercase letters of the alphabet
    prefix_list = [f"<{string.ascii_lowercase[i]}_{{}}>" for i in range(num_prefixes)]

    new_tokens: Set[str] = set()

    for prefix in prefix_list:
        for i in range(code_num):
            token = prefix.format(i)
            new_tokens.add(token)

    new_tokens_list = sorted(list(new_tokens))

    return new_tokens_list


def get_custom_vocab(code_num: int = 256, num_prefixes: int = 4) -> Dict[str, int]:
    """
    Create a custom vocabulary mapping tokens to IDs.
    
    Builds a vocabulary that includes special tokens (pad, eos, unk, mask)
    and all custom code tokens generated by get_all_tokens().
    
    Args:
        code_num: The number of codes to generate for each prefix. Defaults to 256.
        num_prefixes: The number of different prefixes to use. Defaults to 4.
                     Must be between 1 and 26 (inclusive).
    
    Returns:
        A dictionary mapping token strings to their corresponding integer IDs.
        
    Example:
        >>> vocab = get_custom_vocab(2, 2)
        >>> vocab['<pad>']
        0
        >>> vocab['<eos>']
        1
    """
    relevant_tokens = ["<pad>", "<eos>", "<unk>", "<mask>"] + get_all_tokens(code_num, num_prefixes)
    token_to_id = {tok: i for i, tok in enumerate(relevant_tokens)}

    return token_to_id


def save_custom_vocab(code_num: int = 256, num_prefixes: int = 4, filename: str = "custom_tokenizer/custom_vocab.json") -> None:
    """
    Create and save a custom tokenizer with specific vocabulary to a file.
    
    Args:
        code_num: The number of codes to generate for each prefix. Defaults to 256.
        num_prefixes: The number of different prefixes to use. Defaults to 4.
                     Must be between 1 and 26 (inclusive).
        filename: The path where the tokenizer JSON file will be saved. 
                 Defaults to "custom_tokenizer/custom_vocab.json".
    
    Returns:
        None
        
    Raises:
        ValueError: If num_prefixes is not between 1 and 26 (inclusive).
        OSError: If the directory for the filename doesn't exist or is not writable.
    """
    token_to_id = get_custom_vocab(code_num, num_prefixes)

    tokenizer = Tokenizer(WordLevel(token_to_id, unk_token="<unk>"))
    tokenizer.pre_tokenizer = Whitespace()

    tokenizer.save(filename)


def load_custom_tokenizer(filename: str = "custom_tokenizer/custom_vocab.json") -> CustomT5Tokenizer:
    """
    Load a custom tokenizer from a JSON file.
    
    Loads a PreTrainedTokenizerFast from the specified file and configures it
    with special tokens. The tokenizer is then converted to CustomT5Tokenizer class.
    
    Args:
        filename: The path to the tokenizer JSON file. 
                 Defaults to "custom_tokenizer/custom_vocab.json".
    
    Returns:
        A CustomT5Tokenizer instance loaded from the file.
        
    Raises:
        FileNotFoundError: If the tokenizer file doesn't exist.
        ValueError: If the tokenizer file is corrupted or invalid.
        AssertionError: If the pad token ID is not 0.
        
    Example:
        >>> tokenizer = load_custom_tokenizer("my_tokenizer.json")
        >>> tokenizer.pad_token
        '<pad>'
    """
    # NOTE: We will need to add more tokens if we add the image modality.
    tokenizer = PreTrainedTokenizerFast(
        tokenizer_file=filename,
        unk_token="<unk>",
        pad_token="<pad>",
        eos_token="<eos>",
        mask_token="<mask>",
        clean_up_tokenization_spaces=False,
    )

    # Make sure token IDs are compact
    assert tokenizer.convert_tokens_to_ids("<pad>") == 0

    tokenizer.__class__ = CustomT5Tokenizer
    tokenizer.model_input_names = ["input_ids", "attention_mask"]

    return cast(CustomT5Tokenizer, tokenizer)
